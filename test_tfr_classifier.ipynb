{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd9698a3-8ac9-4686-ad86-424357d73d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        inp, hyp, label = row['inp'], row['hyp'], row['label']\n",
    "        prompt = f'PROMPT: {inp} \\n\\n PARTIAL RESPONSE: {hyp}'\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            prompt,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class T5BinaryClassifier(pl.LightningModule):\n",
    "    def __init__(self, model_name='stanfordnlp/SteamSHP-flan-t5-large', learning_rate=3e-5, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        \n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_len = max_len\n",
    "        self.predictions = []\n",
    "        self.labels = []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        if len(input_ids.shape)==1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "            attention_mask = attention_mask.unsqueeze(0)\n",
    "        if labels is not None:\n",
    "            labels = labels.unsqueeze(-1)\n",
    "            return self.model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        else:\n",
    "            return self.model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=2)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['label']\n",
    "        outputs = self(input_ids, attention_mask, labels)\n",
    "        loss = outputs.loss\n",
    "        self.log('train_loss', loss, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['label']\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        if batch_idx%10==0:\n",
    "            print(logits)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "        # self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        #self.log(\"val_accuracy\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "        accuracy = (preds == labels).float().mean()\n",
    "        self.log(\"val_accuracy\", acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        #self.log('val_preds', preds, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        #self.log('val_labels', labels, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        self.predictions.extend(preds.tolist())\n",
    "        self.labels.extend(labels.tolist())\n",
    "        \n",
    "        return {'preds': preds, 'labels': labels}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return AdamW(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "def balance_dataframe(dataframe):\n",
    "    label_counts = dataframe['label'].value_counts()\n",
    "    min_count = label_counts.min()\n",
    "    balanced_data = dataframe.groupby('label').apply(lambda grp: grp.sample(min_count)).reset_index(drop=True)\n",
    "    return balanced_data\n",
    "\n",
    "def train_val_split(dataframe, test_size=0.2, random_state=42):\n",
    "    unique_inp = dataframe['inp'].unique()\n",
    "    train_inp, test_inp = train_test_split(unique_inp, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    train_df = dataframe[dataframe['inp'].isin(train_inp)].reset_index(drop=True)\n",
    "    test_df = dataframe[dataframe['inp'].isin(test_inp)].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def train(dataframe, model_name='t5-small', epochs=2, batch_size=8, learning_rate=3e-5, max_len=512, val_interval=1):\n",
    "    # Balance DataFrame and split into train and test\n",
    "    dataframe = balance_dataframe(dataframe)\n",
    "    train_df, test_df = train_val_split(dataframe)\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    train_dataset = CustomDataset(train_df, tokenizer, max_len)\n",
    "    test_dataset = CustomDataset(test_df, tokenizer, max_len)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=10)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=10)\n",
    "\n",
    "    \n",
    "    model = T5BinaryClassifier(model_name, tokenizer, learning_rate, max_len)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=\"./checkpoints\",\n",
    "        filename=\"{epoch:02d}-{val_accuracy:.4f}\",\n",
    "        save_top_k=2,\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        save_last=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=True,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        # gpus=torch.cuda.device_count(),\n",
    "        log_every_n_steps=val_interval,\n",
    "        #check_val_every_n_epoch=val_interval,\n",
    "        val_check_interval=500,\n",
    "        #callbacks=[checkpoint_callback],\n",
    "        enable_checkpointing=True,\n",
    "        #early_stop_callback=None\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader, ckpt_path=\"./lightning_logs/version_3/checkpoints/epoch=0-step=2000.ckpt\")\n",
    "    \n",
    "def validate(dataframe, model_name='t5-small', epochs=2, batch_size=8, learning_rate=3e-5, max_len=512, val_interval=1):\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    test_dataset = CustomDataset(dataframe, tokenizer, max_len)\n",
    "\n",
    "    val_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=10)\n",
    "\n",
    "    \n",
    "    model = T5BinaryClassifier(model_name, tokenizer, learning_rate, max_len)\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        # gpus=torch.cuda.device_count(),\n",
    "        log_every_n_steps=val_interval,\n",
    "        #check_val_every_n_epoch=val_interval,\n",
    "        val_check_interval=500,\n",
    "        #callbacks=[checkpoint_callback],\n",
    "        enable_checkpointing=True,\n",
    "        #early_stop_callback=None\n",
    "    )\n",
    "    #trainer.fit(model, train_loader, val_loader, ckpt_path=\"./lightning_logs/version_3/checkpoints/epoch=0-step=2000.ckpt\")\n",
    "    trainer.validate(model, val_loader, ckpt_path=\"./lightning_logs/version_4/checkpoints/epoch=2-step=11896.ckpt\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa9f0918-cd3f-4152-94c0-622463898312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5217d492-b5ad-4bc8-85a8-c323d1421916",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7844763934700001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_json(\"output/pfsample/pfsample.jsonl\", lines=True, orient=\"records\")['scos'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0cd461e-71b8-486f-920f-453ae7199903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c14cb425-dae3-46af-b449-4265464d86b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = T5BinaryClassifier.load_from_checkpoint(\"./lightning_logs/version_4/checkpoints/epoch=2-step=11896.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332bbf73-c8d2-4305-85a3-f1221b53f8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qpref.predsingle(\"How is it going? \", \"not going well...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f5eb4-9c11-4cf9-8433-ea73e2747363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace with your actual DataFrame\n",
    "inpdf = pd.read_json(\"std_dataset.jsonl\", lines=True, orient=\"records\")\n",
    "inpdf['label'] = (inpdf['sco']>.05).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc54784-1881-493d-8246-490e21d2c171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_json(\"output/testimp.jsonl\", lines=True, orient=\"records\")\n",
    "test_df['words'] = test_df['prefix'].apply(lambda x: len(x.split()))\n",
    "test_df['hyp']=test_df['prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8c0941f-b31e-4fe3-b197-5494e46be996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'stanfordnlp/SteamSHP-flan-t5-large'\n",
    "max_len = 512\n",
    "learning_rate = 3e-5\n",
    "batch_size=8\n",
    "epochs=1\n",
    "val_interval=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2a0a29b-4688-458e-ad6d-acc7e08c7fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = 41\n",
    "end = 50\n",
    "tmpdf = test_df[test_df['words']<end].reset_index()\n",
    "tmpdf = tmpdf[tmpdf['words']>start].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53921f1f-cd99-43fc-a856-fec2a1e812c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n",
      "10\n",
      "0.30303030303030304\n"
     ]
    }
   ],
   "source": [
    "print(len(tmpdf))\n",
    "print(sum(tmpdf['label']))\n",
    "print(sum(tmpdf['label'])/len(tmpdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89d642f1-e6e7-4435-adac-07e3e0938b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "62b18105-9435-4788-a405-c48411cbb04a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tmpdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 2\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(\u001b[43mtmpdf\u001b[49m, tokenizer, max_len)\n\u001b[1;32m      4\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tmpdf' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "test_dataset = CustomDataset(tmpdf, tokenizer, max_len)\n",
    "\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4d851c0-da39-4089-b533-d20fc90192d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c9c0cf8-d9fa-4a16-8f76-9a83c4d25ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5BinaryClassifier(model_name, tokenizer, learning_rate, max_len)\n",
    "#model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "440f7d11-41aa-44ca-95aa-90ffeab34d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=epochs,\n",
    "    # gpus=torch.cuda.device_count(),\n",
    "    log_every_n_steps=val_interval,\n",
    "    devices=1,\n",
    "    #check_val_every_n_epoch=val_interval,\n",
    "    val_check_interval=500,\n",
    "    #callbacks=[checkpoint_callback],\n",
    "    enable_checkpointing=True,\n",
    "    #early_stop_callback=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1efa20ef-6434-4dc8-aa70-2f83132c1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ./lightning_logs/version_11/checkpoints/epoch=3-step=3359.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Loaded model weights from the checkpoint at ./lightning_logs/version_11/checkpoints/epoch=3-step=3359.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5f7f52268143e896d40e4f3e5100e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy          0.8484848484848485\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_accuracy': 0.8484848484848485}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainer.fit(model, train_loader, val_loader, ckpt_path=\"./lightning_logs/version_3/checkpoints/epoch=0-step=2000.ckpt\")\n",
    "trainer.validate(model, val_loader, ckpt_path=\"./lightning_logs/version_11/checkpoints/epoch=3-step=3359.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ac17762-5cf0-4fe2-abe7-bfff1d8ac47b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(sum(model.predictions))\n",
    "model.predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bc8f5eb-bc0a-4690-9c36-ae72a3e138dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newexs = pd.read_json(\"output/stmshpdset/bigdsetp4.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcf8587f-6e61-4c00-a6bd-5a9f26613d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_prefix_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_rows = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        input_str = row['inp']\n",
    "        hyps = row['hyps']\n",
    "        scos = row['scos']\n",
    "        \n",
    "        for hyp, sco in zip(hyps, scos):\n",
    "            words = hyp.split(' ')\n",
    "            prefix_length = random.randint(1, len(words))\n",
    "            prefix = ' '.join(words[:prefix_length])\n",
    "\n",
    "            new_row = {\n",
    "                'inp': input_str,\n",
    "                'hyp': prefix,\n",
    "                'pflen':prefix_length,\n",
    "                'sco': sco\n",
    "            }\n",
    "            new_rows.append(new_row)\n",
    "\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61d83c6e-52b2-4d17-bc34-4fe4e7b9a724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pfdf = random_prefix_dataframe(newexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c761193-e7b9-4e97-8204-436fdf1bee00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70a3ea94-6ce1-44e6-b46c-ac7327d87035",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inp</th>\n",
       "      <th>hyp</th>\n",
       "      <th>pflen</th>\n",
       "      <th>sco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robert McNamara and why so many people dislike...</td>\n",
       "      <td>He was one of the</td>\n",
       "      <td>5</td>\n",
       "      <td>0.926491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert McNamara and why so many people dislike...</td>\n",
       "      <td>I know he was the</td>\n",
       "      <td>5</td>\n",
       "      <td>0.632656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert McNamara and why so many people dislike...</td>\n",
       "      <td>He is a very controversial figure, and one who...</td>\n",
       "      <td>45</td>\n",
       "      <td>0.862696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robert McNamara and why so many people dislike...</td>\n",
       "      <td>Robert McNamara, a member of the United States...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.608410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert McNamara and why so many people dislike...</td>\n",
       "      <td>He was known for a lot of mistakes and squande...</td>\n",
       "      <td>19</td>\n",
       "      <td>0.798126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>How come people pass out do to an extreme amou...</td>\n",
       "      <td>When</td>\n",
       "      <td>1</td>\n",
       "      <td>0.635739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>How come people pass out do to an extreme amou...</td>\n",
       "      <td>When you experience extreme g-force, you exper...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.820997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>How come people pass out do to an extreme amou...</td>\n",
       "      <td>When you experience extreme g-force, you exper...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.877012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>How come people pass out do to an extreme amou...</td>\n",
       "      <td>When you experience extreme g-force, you exper...</td>\n",
       "      <td>28</td>\n",
       "      <td>0.883407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>How come people pass out do to an extreme amou...</td>\n",
       "      <td>When you experience extreme g-force,</td>\n",
       "      <td>5</td>\n",
       "      <td>0.687428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1928 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    inp  \\\n",
       "0     Robert McNamara and why so many people dislike...   \n",
       "1     Robert McNamara and why so many people dislike...   \n",
       "2     Robert McNamara and why so many people dislike...   \n",
       "3     Robert McNamara and why so many people dislike...   \n",
       "4     Robert McNamara and why so many people dislike...   \n",
       "...                                                 ...   \n",
       "1923  How come people pass out do to an extreme amou...   \n",
       "1924  How come people pass out do to an extreme amou...   \n",
       "1925  How come people pass out do to an extreme amou...   \n",
       "1926  How come people pass out do to an extreme amou...   \n",
       "1927  How come people pass out do to an extreme amou...   \n",
       "\n",
       "                                                    hyp  pflen       sco  \n",
       "0                                     He was one of the      5  0.926491  \n",
       "1                                     I know he was the      5  0.632656  \n",
       "2     He is a very controversial figure, and one who...     45  0.862696  \n",
       "3     Robert McNamara, a member of the United States...     11  0.608410  \n",
       "4     He was known for a lot of mistakes and squande...     19  0.798126  \n",
       "...                                                 ...    ...       ...  \n",
       "1923                                               When      1  0.635739  \n",
       "1924  When you experience extreme g-force, you exper...     12  0.820997  \n",
       "1925  When you experience extreme g-force, you exper...     15  0.877012  \n",
       "1926  When you experience extreme g-force, you exper...     28  0.883407  \n",
       "1927               When you experience extreme g-force,      5  0.687428  \n",
       "\n",
       "[1928 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b2c7ca7-8189-4b47-b3c0-d754b681ceca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_subset(start, end):\n",
    "    tmp = pfdf[pfdf['pflen']>start].reset_index(drop=True).copy()\n",
    "    tmp = tmp[tmp['pflen']<end]\n",
    "    print(len(tmp[tmp['sco']>0.85]))\n",
    "    print(len(tmp))\n",
    "    tmp['label']= (tmp['sco']>0.85).astype(int)\n",
    "    tmp = balance_dataframe(tmp)\n",
    "    tesdataset = CustomDataset(tmp, tokenizer, max_len)\n",
    "\n",
    "    val_loader = DataLoader(tesdataset, batch_size=batch_size, num_workers=10)\n",
    "    #trainer.fit(model, train_loader, val_loader, ckpt_path=\"./lightning_logs/version_3/checkpoints/epoch=0-step=2000.ckpt\")\n",
    "    trainer.validate(model, val_loader)#, ckpt_path=\"./lightning_logs/version_4/checkpoints/epoch=2-step=11896.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8b9fd2b7-8bba-469e-be4d-12a96832235c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592\n",
      "1779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfeff638590496aa3d7a50db2c5267a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], device='cuda:0')\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0]], device='cuda:0')\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], device='cuda:0')\n",
      "tensor([[0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], device='cuda:0')\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], device='cuda:0')\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0]], device='cuda:0')\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], device='cuda:0')\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], device='cuda:0')\n",
      "tensor([[0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0]], device='cuda:0')\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0]], device='cuda:0')\n",
      "tensor([[0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0]], device='cuda:0')\n",
      "tensor([[0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0]], device='cuda:0')\n",
      "tensor([[0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0]], device='cuda:0')\n",
      "tensor([[0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], device='cuda:0')\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 1, 0]], device='cuda:0')\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      val_accuracy          0.7271959459459459\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "valid_subset(0, 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3fea49f1-b093-4a2e-b00c-11289cb34118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predictions[:int(len(model.predictions)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf33741f-850b-469a-bf83-7aec091bd63c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inp</th>\n",
       "      <th>hyp</th>\n",
       "      <th>pflen</th>\n",
       "      <th>sco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robert McNamara and why so many people dislike...</td>\n",
       "      <td>He was one of the most</td>\n",
       "      <td>6</td>\n",
       "      <td>0.926491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert McNamara and why so many people dislike...</td>\n",
       "      <td>I know he was the secretary of defense during ...</td>\n",
       "      <td>25</td>\n",
       "      <td>0.632656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Robert McNamara and why so many people dislike...</td>\n",
       "      <td>He is a very controversial figure, and one who...</td>\n",
       "      <td>48</td>\n",
       "      <td>0.862696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robert McNamara and why so many people dislike...</td>\n",
       "      <td>Robert McNamara, a member of the United States...</td>\n",
       "      <td>15</td>\n",
       "      <td>0.608410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Robert McNamara and why so many people dislike...</td>\n",
       "      <td>He was known for a lot of mistakes and squande...</td>\n",
       "      <td>24</td>\n",
       "      <td>0.798126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>How come people pass out do to an extreme amou...</td>\n",
       "      <td>When you experience extreme g-force, you exper...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.635739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>How come people pass out do to an extreme amou...</td>\n",
       "      <td>When you experience extreme g-force, you exper...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.820997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>How come people pass out do to an extreme amou...</td>\n",
       "      <td>When you experience extreme g-force, you exper...</td>\n",
       "      <td>11</td>\n",
       "      <td>0.877012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>How come people pass out do to an extreme amou...</td>\n",
       "      <td>When you experience extreme g-force, you exper...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.883407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>How come people pass out do to an extreme amou...</td>\n",
       "      <td>When you experience extreme g-force, you exper...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.687428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    inp  \\\n",
       "0     Robert McNamara and why so many people dislike...   \n",
       "1     Robert McNamara and why so many people dislike...   \n",
       "2     Robert McNamara and why so many people dislike...   \n",
       "3     Robert McNamara and why so many people dislike...   \n",
       "4     Robert McNamara and why so many people dislike...   \n",
       "...                                                 ...   \n",
       "1923  How come people pass out do to an extreme amou...   \n",
       "1924  How come people pass out do to an extreme amou...   \n",
       "1925  How come people pass out do to an extreme amou...   \n",
       "1926  How come people pass out do to an extreme amou...   \n",
       "1927  How come people pass out do to an extreme amou...   \n",
       "\n",
       "                                                    hyp  pflen       sco  \n",
       "0                                He was one of the most      6  0.926491  \n",
       "1     I know he was the secretary of defense during ...     25  0.632656  \n",
       "2     He is a very controversial figure, and one who...     48  0.862696  \n",
       "3     Robert McNamara, a member of the United States...     15  0.608410  \n",
       "4     He was known for a lot of mistakes and squande...     24  0.798126  \n",
       "...                                                 ...    ...       ...  \n",
       "1923  When you experience extreme g-force, you exper...     11  0.635739  \n",
       "1924  When you experience extreme g-force, you exper...      9  0.820997  \n",
       "1925  When you experience extreme g-force, you exper...     11  0.877012  \n",
       "1926  When you experience extreme g-force, you exper...     33  0.883407  \n",
       "1927  When you experience extreme g-force, you exper...     12  0.687428  \n",
       "\n",
       "[1700 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfdf[pfdf['pflen']<60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47dd625-a0dc-44b5-9573-53f939551a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
