{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb47bb9b-43e6-4c58-9952-8c4e524ddcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasann/miniconda3/envs/aplora/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/prasann/miniconda3/envs/aplora/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda120.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 120\n",
      "CUDA SETUP: Loading binary /home/prasann/miniconda3/envs/aplora/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda120.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prasann/miniconda3/envs/aplora/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/prasann/miniconda3/envs/aplora did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/prasann/miniconda3/envs/aplora/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from alpaca_farm.models import reward_model\n",
    "from alpaca_farm.inference.decode import load_model_and_tokenizer_for_inference \n",
    "from alpaca_farm.inference.score import score_sequences_with_huggingface_given_model\n",
    "from alpaca_farm import utils\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a969ea27-d049-4279-8f5f-bf4d6768288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aa1bb65-c0e6-4bb3-a4e1-007b0aac289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = f'{46000}MB'\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "device_map = \"auto\"\n",
    "\n",
    "# if we are in a distributed setting, we need to set the device map and max memory per device\n",
    "if os.environ.get('LOCAL_RANK') is not None:\n",
    "    local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n",
    "    device_map = {'': local_rank}\n",
    "    max_memory = {'': max_memory[local_rank]}\n",
    "# normal llama: \n",
    "# mpath = \"/home/prasann/Projects/tfr-decoding/llama/llama\"\n",
    "# PPO: \n",
    "mpath = \"/home/prasann/Projects/tfr-decoding/apfarm_models/ppo-human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f0046-e174-4867-b691-d48c2f61c6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=mpath,\n",
    "    load_in_4bit=True,\n",
    "    device_map=0,\n",
    "    max_memory=max_memory,\n",
    "    #low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float32,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4'\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa50611-f320-4085-92ce-8866b62a97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = LlamaTokenizer.from_pretrained(mpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f45d817f-94c9-48b2-9f0d-c00f4a840688",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give a long and detailed response that answers the question. \\n Question: Why can't penguins fly? \\nResponse: \"\n",
    "inps = tok(prompt, return_tensors='pt').to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f189d454-faf7-422f-9187-87dfccba9798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> Give a long and detailed response that answers the question. \\n Question: Why can't penguins fly? \\nResponse: \\nPenguins are unable to fly due to their body structure. Their wings are too heavy and their body is not warm enough to generate enough energy for flight. Penguins also lack the trait of flexibility needed to make tight turns in flight, making it impossible for them to maintain flight for long periods of time. Additionally, penguins have adapted to live in the cold climate of the Antarctic, and warm body temperature is required for flight. Lastly, penguins lack the extra layer of body fat that helps to provide buoyancy in the air, making it difficult for them to stay afloat while attempting to fly.  In short, penguins' bodies are simply not designed for flight.</s>\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.batch_decode(model.generate(**inps, max_new_tokens=200, do_sample=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e92ce2-b098-43fa-97bd-be079a7df436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One of the `_name_or_path` to the root node is not a pretrained model name or local path on disk. This may be due to copying checkpoints across machines. Falling back to hardcoded rule to figure out the pretrained model based on config.json.\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:10<00:00,  3.60s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.85s/it]\n"
     ]
    }
   ],
   "source": [
    "model = reward_model.RewardModel.from_pretrained(\n",
    "    \"/home/prasann/Projects/tfr-decoding/apfarm_models/reward-model-human/\",\n",
    "    load_in_4bit=True,\n",
    "    device_map=0,\n",
    "    max_memory=max_memory,\n",
    "    #low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    flash_attn=False,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float32,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4'\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f75cab-3eca-4f72-8644-97a5ebe289e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model for inference: /home/prasann/Projects/tfr-decoding/apfarm_models/reward-model-human/\n",
      "One of the `_name_or_path` to the root node is not a pretrained model name or local path on disk. This may be due to copying checkpoints across machines. Falling back to hardcoded rule to figure out the pretrained model based on config.json.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4bit quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:14<00:00,  4.92s/it]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rm, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_and_tokenizer_for_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/prasann/Projects/tfr-decoding/apfarm_models/reward-model-human/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRewardModel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_str_dtype_to_torch_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflash_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users/prasann/Projects/tfr-decoding/alpaca_farm/src/alpaca_farm/inference/decode.py:82\u001b[0m, in \u001b[0;36mload_model_and_tokenizer_for_inference\u001b[0;34m(model_name_or_path, cache_dir, model_cls, model_kwargs, tokenizer_kwargs, resize_token_embeddings_if_mismatch)\u001b[0m\n\u001b[1;32m     79\u001b[0m     default_tokenizer_kwargs\u001b[38;5;241m.\u001b[39mupdate(tokenizer_kwargs)\n\u001b[1;32m     80\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m default_tokenizer_kwargs\n\u001b[0;32m---> 82\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     83\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# base llama does not come with a pad token, possible for other pretrained models as well\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/aplora/lib/python3.9/site-packages/transformers/modeling_utils.py:2672\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2672\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2674\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m/data/users/prasann/Projects/tfr-decoding/alpaca_farm/src/alpaca_farm/models/reward_model.py:48\u001b[0m, in \u001b[0;36mRewardModel.__init__\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mzeros_(reward_head\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# TODO is this adjustment acceptable?\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# self.reward_head = reward_head.to(next(self.backbone_model.parameters()).device)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_head \u001b[38;5;241m=\u001b[39m \u001b[43mreward_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aplora/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/aplora/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/aplora/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "rm, tokenizer = load_model_and_tokenizer_for_inference(\n",
    "    model_name_or_path=\"/home/prasann/Projects/tfr-decoding/apfarm_models/reward-model-human/\",\n",
    "    model_cls=reward_model.RewardModel,\n",
    "    cache_dir=None,\n",
    "    model_kwargs=dict(\n",
    "        torch_dtype=utils.convert_str_dtype_to_torch_dtype(None),\n",
    "        flash_attn=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2caef544-1a94-4a99-b9e9-f1398f4829ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreseq(inp, ans):\n",
    "    # TODO make a batched version if necessary\n",
    "    template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:{response}\"\n",
    "    seq = template.format(instruction=inp, response=ans)\n",
    "    return score_sequences_with_huggingface_given_model(model, tok, [seq], per_device_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0365aef-5e02-4479-8304-71226676c51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown mixed precision mode: None, falling back to fp32.\n",
      "mixed_precision = None\n",
      "evaluating rewards for batches: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7752610445022583]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoreseq(\"Why can't penguins fly?\", \"Penguins are unable to fly due to their body structure. Their wings are too heavy and their body is not warm enough to generate enough energy for flight. Penguins also lack the trait of flexibility needed to make tight turns in flight, making it impossible for them to maintain flight for long periods of time. Additionally, penguins have adapted to live in the cold climate of the Antarctic, and warm body temperature is required for flight. Lastly, penguins lack the extra layer of body fat that helps to provide buoyancy in the air, making it difficult for them to stay afloat while attempting to fly.  In short, penguins' bodies are simply not designed for flight.\")\n",
    "# 1.727 from normal RM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c728e1-ad02-49f7-9966-35e7a4d343d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
